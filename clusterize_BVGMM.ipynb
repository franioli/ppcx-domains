{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bc2b925",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import joblib\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.mixture import BayesianGaussianMixture\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "from src.clustering import (\n",
    "    plot_gmm_clusters,\n",
    "    plot_gmm_log_likelihood_contours,\n",
    "    preproc_features,\n",
    ")\n",
    "from src.config import ConfigManager\n",
    "from src.database import get_dic_analysis_ids, get_dic_data, get_image\n",
    "from src.roi import PolygonROISelector, filter_dataframe\n",
    "from src.visualization import plot_dic_vectors\n",
    "\n",
    "%matplotlib widget\n",
    "\n",
    "config = ConfigManager()\n",
    "\n",
    "# Parameters for DIC data processing\n",
    "# camera_names = [\"PPCX_Tele\", \"PPCX_Wide\"]\n",
    "# Use only one camera for testing\n",
    "camera_names = config.get(\"clustering.variables_names\")\n",
    "\n",
    "min_velocity = config.get(\"dic.min_velocity\")\n",
    "filter_outliers = config.get(\"dic.filter_outliers\")\n",
    "tails_percentile = config.get(\"dic.tails_percentile\")\n",
    "\n",
    "# Parameters for GMM clustering\n",
    "variables = config.get(\"clustering.variables_names\")\n",
    "n_components = config.get(\"clustering.n_components\")\n",
    "weight_concentration_prior = config.get(\"clustering.weight_concentration_prior\")\n",
    "\n",
    "# Parameters data selection and output\n",
    "target_date = \"2024-09-02\"\n",
    "camera_name = \"PPCX_Tele\"\n",
    "base_output_dir = \"output\"\n",
    "\n",
    "# Create the connection to the database\n",
    "db_engine = create_engine(config.db_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963264fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.get(\"dic.min_velocity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98933b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the output folder and base_name\n",
    "output_dir = Path(base_output_dir) / camera_name\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "base_name = f\"{camera_name}_{target_date}_GMM\"\n",
    "\n",
    "# Get DIC analysis metadata (filtered by date/camera)\n",
    "dic_analyses = get_dic_analysis_ids(\n",
    "    db_engine, reference_date=target_date, camera_name=camera_name\n",
    ")\n",
    "dic_analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a923273",
   "metadata": {},
   "outputs": [],
   "source": [
    "master_image_id = dic_analyses[\"master_image_id\"].iloc[0]\n",
    "master_image_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c05416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the master image for the DIC analysis via the API\n",
    "\n",
    "img = get_image(master_image_id, camera_name=camera_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39de501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the displacement data for that DIC analysis via the API\n",
    "dic_id = dic_analyses[\"dic_id\"].iloc[0]\n",
    "df = get_dic_data(\n",
    "    dic_id,\n",
    "    filter_outliers=filter_outliers,\n",
    "    tails_percentile=tails_percentile,\n",
    "    min_velocity=min_velocity,\n",
    "    app_host=config.get(\"api.host\"),\n",
    "    app_port=config.get(\"api.port\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e39aad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the selector from a saved polygon\n",
    "selector = PolygonROISelector.from_file(\n",
    "    \"data/PPCX_Tele_glacier_ROI.json\",\n",
    ")\n",
    "df = filter_dataframe(\n",
    "    df,\n",
    "    selector.polygon_path,\n",
    "    x_col=\"x\",\n",
    "    y_col=\"y\",\n",
    ")\n",
    "# visualize_polygon_filter(\n",
    "#     df,\n",
    "#     selector2,\n",
    "#     img=img,\n",
    "#     figsize=(4, 5),\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ade948",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "plot_dic_vectors(\n",
    "    x=df[\"x\"].to_numpy(),\n",
    "    y=df[\"y\"].to_numpy(),\n",
    "    u=df[\"u\"].to_numpy(),\n",
    "    v=df[\"v\"].to_numpy(),\n",
    "    magnitudes=df[\"V\"].to_numpy(),\n",
    "    background_image=img,\n",
    "    cmap_name=\"batlow\",\n",
    "    # vmin=0.1,\n",
    "    # vmax=10,\n",
    "    fig=fig,\n",
    "    ax=ax,\n",
    ")\n",
    "fig.savefig(output_dir / f\"{base_name}_dic.png\", dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1236e301",
   "metadata": {},
   "source": [
    "Variational Bayesian Gaussian Mixture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67650847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features and standardize them\n",
    "df_features = preproc_features(df)\n",
    "features = df_features[variables_names].values\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "# Apply weights\n",
    "for i, var_name in enumerate(variables_names):\n",
    "    weight = feature_weights.get(var_name, 1.0)\n",
    "    features_scaled[:, i] *= weight\n",
    "\n",
    "\n",
    "gmm = BayesianGaussianMixture(\n",
    "    n_components=n_components,\n",
    "    weight_concentration_prior=weight_concentration_prior,\n",
    "    covariance_type=config.get(\"clustering.covariance_type\"),\n",
    "    max_iter=config.get(\"clustering.max_iter\"),\n",
    "    random_state=config.get(\"clustering.random_state\",\n",
    ")\n",
    "gmm.fit(features_scaled)\n",
    "labels = gmm.predict(features_scaled)\n",
    "\n",
    "\n",
    "fig, ax, stats_df = plot_gmm_clusters(\n",
    "    df_features,\n",
    "    labels,\n",
    "    var_names=[\"V\", \"angle_rad\"],\n",
    "    img=img,\n",
    "    figsize=(8, 6),\n",
    ")\n",
    "fig.savefig(output_dir / f\"{base_name}_clusters.png\", dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "# Save the GMM model\n",
    "joblib.dump(scaler, output_dir / f\"{base_name}_scaler.joblib\")\n",
    "gmm_run_name = f\"{base_name}_GMM_comp{n_components}_cov{covariance_type}_wcp{weight_concentration_prior}\"\n",
    "joblib.dump(gmm, output_dir / f\"{gmm_run_name}.joblib\")\n",
    "\n",
    "# Save the features DataFrame with labels\n",
    "df_features.to_csv(output_dir / f\"{base_name}_features_with_labels.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1673afb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features and standardize them\n",
    "df_features = preproc_features(df)\n",
    "features = df_features[variables_names].values\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "# Apply weights\n",
    "for i, var_name in enumerate(variables_names):\n",
    "    weight = feature_weights.get(var_name, 1.0)\n",
    "    features_scaled[:, i] *= weight\n",
    "\n",
    "\n",
    "gmm = BayesianGaussianMixture(\n",
    "    n_components=5,\n",
    "    weight_concentration_prior=0.001,\n",
    "    covariance_type=covariance_type,\n",
    "    max_iter=max_iter,\n",
    "    random_state=random_state,\n",
    ")\n",
    "gmm.fit(features_scaled)\n",
    "labels = gmm.predict(features_scaled)\n",
    "\n",
    "\n",
    "fig, ax, stats_df = plot_gmm_clusters(\n",
    "    df_features,\n",
    "    labels,\n",
    "    var_names=[\"V\", \"angle_rad\"],\n",
    "    img=img,\n",
    "    figsize=(8, 6),\n",
    ")\n",
    "fig.savefig(output_dir / f\"{base_name}_clusters.png\", dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "# Save the GMM model\n",
    "joblib.dump(scaler, output_dir / f\"{base_name}_scaler.joblib\")\n",
    "gmm_run_name = f\"{base_name}_GMM_comp{n_components}_cov{covariance_type}_wcp{weight_concentration_prior}\"\n",
    "joblib.dump(gmm, output_dir / f\"{gmm_run_name}.joblib\")\n",
    "\n",
    "# Save the features DataFrame with labels\n",
    "df_features.to_csv(output_dir / f\"{base_name}_features_with_labels.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a2112d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gmm_log_likelihood_contours(\n",
    "    df_features, gmm, scaler, variables_names, pair=(\"V\", \"angle_rad\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6096f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- Post-process clustering ---\n",
    "# min_cluster_size = 30\n",
    "# smoothing_window_size = 10\n",
    "# labels_clean = remove_small_clusters(labels, min_size=min_cluster_size)\n",
    "# labels_smooth = spatial_smooth_labels(\n",
    "#     df_features, labels_clean, window_size=smoothing_window_size\n",
    "# )\n",
    "# labels_smooth = merge_similar_clusters(df_features, labels_smooth, threshold=10)\n",
    "\n",
    "# # Plot results after cleaning and smoothing\n",
    "# plot_gmm_clusters(\n",
    "#     df_features,\n",
    "#     labels_smooth,\n",
    "#     var_names=[\"V\", \"angle_rad\"],\n",
    "#     img=img,\n",
    "#     figsize=(8, 6),\n",
    "# )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ppcx-domains (3.11.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
