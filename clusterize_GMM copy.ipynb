{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8bc2b925",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import io\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "from PIL import Image\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from src.database import DICdb\n",
    "\n",
    "%matplotlib widget\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "DB_HOST = os.environ.get(\"DB_HOST\", \"150.145.51.193\")\n",
    "DB_PORT = os.environ.get(\"DB_PORT\", \"5432\")\n",
    "DB_NAME = os.environ.get(\"DB_NAME\", \"planpincieux\")\n",
    "DB_USER = os.environ.get(\"DB_USER\", \"\")\n",
    "DB_PASSWORD = os.environ.get(\"DB_PASSWORD\", \"\")\n",
    "\n",
    "APP_HOST = \"150.145.51.193\"\n",
    "APP_PORT = 8001\n",
    "IMG_GET_API_BASEURL = f\"http://{DB_HOST}:{APP_PORT}/pics/image/\"\n",
    "\n",
    "db = DICdb(\n",
    "    host=DB_HOST, port=DB_PORT, database=DB_NAME, user=DB_USER, password=DB_PASSWORD\n",
    ")\n",
    "\n",
    "target_date = \"2024-08-23\"\n",
    "dic_analysis_id = 3096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a16bd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get DIC data for the target date\n",
    "dic_data = db.get_dic_data(target_date=target_date)\n",
    "\n",
    "# Filter a specific DIC analysis\n",
    "dic_data = dic_data[dic_data[\"analysis_id\"] == dic_analysis_id]\n",
    "\n",
    "# Get the master image for the DIC analysis\n",
    "master_image_id = dic_data[\"master_image_id\"].iloc[0]\n",
    "response = requests.get(IMG_GET_API_BASEURL + f\"{master_image_id}/\")\n",
    "img = Image.open(io.BytesIO(response.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ec03365",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_extract = {\n",
    "    \"seed_x_px\": \"x\",\n",
    "    \"seed_y_px\": \"y\",\n",
    "    \"displacement_x_px\": \"u\",\n",
    "    \"displacement_y_px\": \"v\",\n",
    "    \"displacement_magnitude_px\": \"V\",\n",
    "}\n",
    "df = dic_data[columns_to_extract.keys()].rename(columns=columns_to_extract)\n",
    "\n",
    "# Filter out low velocity vectors\n",
    "min_velocity = 0.5  # Minimum velocity threshold in pixels\n",
    "df = df[df[\"V\"] >= min_velocity].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42d825b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc_features(df):\n",
    "    \"\"\"\n",
    "    Create additional features for clustering glacier displacement data\n",
    "    \"\"\"\n",
    "    df_features = df.copy()\n",
    "\n",
    "    # Direction angle (in radians and degrees)\n",
    "    df_features[\"angle_rad\"] = np.arctan2(df[\"v\"], df[\"u\"])\n",
    "    df_features[\"angle_deg\"] = np.degrees(df_features[\"angle_rad\"])\n",
    "\n",
    "    # Convert negative angles to positive (0-360 degrees)\n",
    "    df_features[\"angle_deg\"] = (df_features[\"angle_deg\"] + 360) % 360\n",
    "\n",
    "    # Directional components (unit vectors)\n",
    "    df_features[\"u_unit\"] = df[\"u\"] / (df[\"V\"] + 1e-10)  # Avoid division by zero\n",
    "    df_features[\"v_unit\"] = df[\"v\"] / (df[\"V\"] + 1e-10)\n",
    "\n",
    "    # Spatial derivatives (approximate gradients)\n",
    "    # Note: This is a simplified approach - in practice you might want to use proper spatial interpolation\n",
    "    df_features[\"spatial_index\"] = range(len(df_features))\n",
    "\n",
    "    # Log magnitude for better clustering of different scales\n",
    "    df_features[\"log_magnitude\"] = np.log1p(df[\"V\"])\n",
    "\n",
    "    # Magnitude categories\n",
    "    magnitude_percentiles = np.percentile(df[\"V\"], [25, 50, 75])\n",
    "    df_features[\"magnitude_category\"] = pd.cut(\n",
    "        df[\"V\"],\n",
    "        bins=[0] + list(magnitude_percentiles) + [np.inf],\n",
    "        labels=[\"low\", \"medium\", \"high\", \"very_high\"],\n",
    "    )\n",
    "\n",
    "    return df_features\n",
    "\n",
    "\n",
    "def normalize_data(\n",
    "    df,\n",
    "    spatial_weight=0.3,\n",
    "    velocity_weight=0.7,\n",
    "    spatial_features_names=None,\n",
    "    velocity_features_names=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Custom clustering approach that considers both spatial proximity and velocity similarity\n",
    "\n",
    "    Parameters:\n",
    "    - spatial_weight: weight for spatial coordinates (x, y)\n",
    "    - velocity_weight: weight for velocity features (u, v, magnitude, direction)\n",
    "    \"\"\"\n",
    "\n",
    "    if spatial_features_names is None:\n",
    "        spatial_features_names = [\"x\", \"y\"]\n",
    "    if velocity_features_names is None:\n",
    "        velocity_features_names = [\"u\", \"v\", \"V\", \"angle_deg\"]\n",
    "\n",
    "    # Prepare features for clustering\n",
    "    spatial_features = df[spatial_features_names].values\n",
    "    velocity_features = df[velocity_features_names].values\n",
    "\n",
    "    # Normalize features separately\n",
    "    spatial_scaler = StandardScaler()\n",
    "    velocity_scaler = StandardScaler()\n",
    "\n",
    "    spatial_normalized = spatial_scaler.fit_transform(spatial_features)\n",
    "    velocity_normalized = velocity_scaler.fit_transform(velocity_features)\n",
    "\n",
    "    # Create a DataFrame with normalized features\n",
    "    normalized_df = pd.DataFrame(\n",
    "        np.hstack(\n",
    "            [\n",
    "                spatial_normalized * spatial_weight,\n",
    "                velocity_normalized * velocity_weight,\n",
    "            ]\n",
    "        ),\n",
    "        columns=spatial_features_names + velocity_features_names,\n",
    "    )\n",
    "\n",
    "    return normalized_df, spatial_scaler, velocity_scaler\n",
    "\n",
    "\n",
    "def visualize_uv_plt(df, ax=None, **kwargs):\n",
    "    \"\"\"\n",
    "    Visualize the u-v scatter plot with optional background image.\n",
    "    \"\"\"\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "    V = df[\"V\"].values if \"V\" in df else np.sqrt(df[\"u\"] ** 2 + df[\"v\"] ** 2)\n",
    "    scatter = ax.scatter(\n",
    "        df[\"u\"], df[\"v\"], s=1, c=V, alpha=0.6, cmap=\"viridis\", **kwargs\n",
    "    )\n",
    "    ax.set_xlabel(\"u (displacement in x direction)\")\n",
    "    ax.set_ylabel(\"v (displacement in y direction)\")\n",
    "    ax.set_title(\"Displacement Vectors (u-v Scatter Plot)\")\n",
    "    plt.colorbar(scatter, ax=ax)\n",
    "    ax.set_aspect(\"equal\", adjustable=\"box\")\n",
    "\n",
    "\n",
    "def visualize_pca(df, columns_to_extract=None, normalize=False):\n",
    "    \"visualize the enhanced features using PCA for dimensionality reduction\"\n",
    "    from sklearn.decomposition import PCA\n",
    "\n",
    "    # Prepare data for PCA\n",
    "    if columns_to_extract is None:\n",
    "        columns_to_extract = [\"x\", \"y\", \"u\", \"v\", \"V\", \"angle_deg\"]\n",
    "\n",
    "    # Ensure all required columns are present in the DataFrame\n",
    "    missing_columns = set(columns_to_extract) - set(df.columns)\n",
    "    if missing_columns:\n",
    "        raise ValueError(f\"Missing columns in DataFrame: {missing_columns}\")\n",
    "\n",
    "    if normalize:\n",
    "        scaler = StandardScaler()\n",
    "        data = scaler.fit_transform(df[columns_to_extract])\n",
    "    else:\n",
    "        data = df[columns_to_extract].values\n",
    "\n",
    "    # Reduce to 2D using PCA\n",
    "    pca = PCA(n_components=2)\n",
    "    reduced_data = pca.fit_transform(data)\n",
    "    df_reduced = pd.DataFrame(reduced_data, columns=[\"PC1\", \"PC2\"])\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 5))\n",
    "    ax.scatter(df_reduced[\"PC1\"], df_reduced[\"PC2\"], s=1, alpha=0.6)\n",
    "    ax.set_xlabel(\"Principal Component 1\")\n",
    "    ax.set_ylabel(\"Principal Component 2\")\n",
    "    ax.set_title(\"PCA Reduced Features Scatter Plot\")\n",
    "    ax.set_aspect(\"equal\", adjustable=\"box\")\n",
    "\n",
    "\n",
    "def plot_clustering_results(df, labels, var_names=None, img=None, figsize=(15, 10)):\n",
    "    \"\"\"\n",
    "    Visualize clustering results on the glacier displacement field.\n",
    "    Simplified version inspired by plot_dic_vectors function.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=figsize)\n",
    "\n",
    "    # Get unique labels and create colors\n",
    "    unique_labels = np.unique(labels)\n",
    "    n_clusters = len(unique_labels[unique_labels >= 0])  # Exclude noise (-1)\n",
    "\n",
    "    # Use a colormap for consistent colors\n",
    "    if n_clusters > 0:\n",
    "        cmap = plt.get_cmap(\"Set3\")\n",
    "        colors = cmap(np.linspace(0, 1, max(n_clusters, 3)))\n",
    "        # Handle noise points with red color\n",
    "        color_map = {}\n",
    "        cluster_idx = 0\n",
    "        for label in unique_labels:\n",
    "            if label == -1:\n",
    "                color_map[label] = \"red\"\n",
    "            else:\n",
    "                color_map[label] = colors[cluster_idx % len(colors)]\n",
    "                cluster_idx += 1\n",
    "    else:\n",
    "        color_map = {-1: \"red\"}\n",
    "\n",
    "    # Plot 1: Spatial distribution of clusters\n",
    "    ax1 = axes[0, 0]\n",
    "    for label in unique_labels:\n",
    "        mask = labels == label\n",
    "        if np.any(mask):\n",
    "            cluster_name = \"Noise\" if label == -1 else f\"Cluster {label}\"\n",
    "            ax1.scatter(\n",
    "                df.loc[mask, \"x\"],\n",
    "                df.loc[mask, \"y\"],\n",
    "                c=[color_map[label]],\n",
    "                s=2,\n",
    "                alpha=0.7,\n",
    "                label=cluster_name,\n",
    "            )\n",
    "\n",
    "    ax1.set_title(\"Spatial Distribution\")\n",
    "    ax1.invert_yaxis()  # Match image coordinates\n",
    "    ax1.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "    ax1.set_aspect(\"equal\")\n",
    "    ax1.grid(False)\n",
    "    ax1.set_xlabel(\"\")\n",
    "    ax1.set_ylabel(\"\")\n",
    "    ax1.set_xticks([])\n",
    "    ax1.set_yticks([])\n",
    "\n",
    "    # Plot 2: Displacement vectors colored by velocity magnitude (like plot_dic_vectors)\n",
    "    ax2 = axes[0, 1]\n",
    "    if img is not None:\n",
    "        ax2.imshow(img, alpha=0.7)\n",
    "    q = ax2.quiver(\n",
    "        df[\"x\"],\n",
    "        df[\"y\"],\n",
    "        df[\"u\"],\n",
    "        df[\"v\"],\n",
    "        df[\"V\"],  # Color by velocity magnitude\n",
    "        scale=None,\n",
    "        scale_units=\"xy\",\n",
    "        angles=\"xy\",\n",
    "        cmap=\"viridis\",\n",
    "        width=0.003,\n",
    "        headwidth=2.5,\n",
    "        alpha=0.8,\n",
    "    )\n",
    "    cbar = fig.colorbar(q, ax=ax2)\n",
    "    cbar.set_label(\"Displacement Magnitude (pixels)\")\n",
    "    ax2.set_title(\"Displacement Vectors\")\n",
    "    ax2.grid(False)\n",
    "    ax2.set_aspect(\"equal\")\n",
    "    ax2.set_xlabel(\"\")\n",
    "    ax2.set_ylabel(\"\")\n",
    "    ax2.set_xticks([])\n",
    "    ax2.set_yticks([])\n",
    "\n",
    "    # Plot 3: 2D scatter plot of two variables (e.g., angle vs magnitude)\n",
    "    ax3 = axes[1, 0]\n",
    "    if var_names is None:\n",
    "        var_names = [\"angle_deg\", \"V\"]\n",
    "    if len(var_names) != 2:\n",
    "        raise ValueError(\n",
    "            \"var_names must contain exactly two variable names for the 2D scatter plot.\"\n",
    "        )\n",
    "    if not all(var in df.columns for var in var_names):\n",
    "        raise ValueError(\n",
    "            f\"One or both of the specified variables {var_names} do not exist in the DataFrame.\"\n",
    "        )\n",
    "    for label in unique_labels:\n",
    "        mask = labels == label\n",
    "        if np.any(mask):\n",
    "            cluster_name = \"Noise\" if label == -1 else f\"Cluster {label}\"\n",
    "            ax3.scatter(\n",
    "                df.loc[mask, var_names[0]],\n",
    "                df.loc[mask, var_names[1]],\n",
    "                c=[color_map[label]],\n",
    "                s=15,\n",
    "                alpha=0.6,\n",
    "                label=cluster_name,\n",
    "            )\n",
    "    ax3.set_title(\"2D Scatter Plot\")\n",
    "    ax3.set_xlabel(var_names[0])\n",
    "    ax3.set_ylabel(var_names[1])\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    ax3.legend()\n",
    "\n",
    "    # Plot 4: Cluster statistics\n",
    "    ax4 = axes[1, 1]\n",
    "    cluster_stats = []\n",
    "    for label in unique_labels:\n",
    "        mask = labels == label\n",
    "        cluster_name = \"Noise\" if label == -1 else f\"Cluster {label}\"\n",
    "\n",
    "        stats = {\n",
    "            \"Cluster\": cluster_name,\n",
    "            \"Count\": np.sum(mask),\n",
    "            \"Mean_Magnitude\": df.loc[mask, \"V\"].mean(),\n",
    "            \"Mean_Direction\": df.loc[mask, \"angle_deg\"].mean(),\n",
    "            \"Std_Magnitude\": df.loc[mask, \"V\"].std(),\n",
    "        }\n",
    "        cluster_stats.append(stats)\n",
    "    stats_df = pd.DataFrame(cluster_stats)\n",
    "\n",
    "    # Bar plot of cluster sizes\n",
    "    bars = ax4.bar(\n",
    "        range(len(stats_df)),\n",
    "        stats_df[\"Count\"],\n",
    "        color=[color_map[label] for label in unique_labels],\n",
    "    )\n",
    "\n",
    "    ax4.set_title(\"Cluster Sizes\")\n",
    "    ax4.set_xlabel(\"Cluster\")\n",
    "    ax4.set_ylabel(\"Number of Points\")\n",
    "    ax4.set_xticks(range(len(stats_df)))\n",
    "    ax4.set_xticklabels(stats_df[\"Cluster\"], rotation=45)\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "56a22e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features = preproc_features(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4a360765",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_mixture_clustering(\n",
    "    df, variables_names=None, n_components=4, max_iter=100, random_state=42, **kwargs\n",
    "):\n",
    "    \"\"\"\n",
    "    Simple Gaussian Mixture clustering for glacier displacement data.\n",
    "    Returns cluster labels for each point.\n",
    "    \"\"\"\n",
    "    if variables_names is None:\n",
    "        variables_names = [\"V\", \"angle_deg\"]\n",
    "    features = df[variables_names].values\n",
    "    scaler = StandardScaler()\n",
    "    features_scaled = scaler.fit_transform(features)\n",
    "    gmm = GaussianMixture(\n",
    "        n_components=n_components,\n",
    "        max_iter=max_iter,\n",
    "        random_state=random_state,\n",
    "        **kwargs,\n",
    "    )\n",
    "    gmm.fit(features_scaled)\n",
    "    labels = gmm.predict(features_scaled)\n",
    "    return labels\n",
    "\n",
    "\n",
    "# --- Run GMM clustering and plot ---\n",
    "# variables_names = [\"V\", \"angle_deg\"]\n",
    "# n_components = 8\n",
    "# gmm_labels = gaussian_mixture_clustering(\n",
    "#     df_features, variables_names=variables_names, n_components=n_components\n",
    "# )\n",
    "# plot_clustering_results(\n",
    "#     df_features,\n",
    "#     gmm_labels,\n",
    "#     var_names=variables_names if len(variables_names) == 2 else [\"V\", \"angle_deg\"],\n",
    "#     img=img,\n",
    "#     figsize=(12, 8),\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "16dd3367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Run GMM clustering and plot ---\n",
    "variables_names = [\"x\", \"y\", \"V\", \"angle_rad\"]\n",
    "n_components = 6\n",
    "max_iter = 100\n",
    "random_state = 42\n",
    "covariance_type = \"full\"\n",
    "\n",
    "features = df_features[variables_names].values\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "# features_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fd94bfa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm = GaussianMixture(\n",
    "    n_components=n_components,\n",
    "    covariance_type=covariance_type,\n",
    "    max_iter=max_iter,\n",
    "    random_state=random_state,\n",
    ")\n",
    "gmm.fit(features_scaled)\n",
    "labels = gmm.predict(features_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d4c4e299",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "X has 2 features, but GaussianMixture is expecting 4 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      8\u001b[39m X, Y = np.meshgrid(x, y)\n\u001b[32m      9\u001b[39m XX = np.array([X.ravel(), Y.ravel()]).T\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m Z = -\u001b[43mgmm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscore_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m Z = Z.reshape(X.shape)\n\u001b[32m     13\u001b[39m fig, ax = plt.subplots(figsize=(\u001b[32m8\u001b[39m, \u001b[32m5\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cnr/ppcx-app/.venv/lib/python3.12/site-packages/sklearn/mixture/_base.py:352\u001b[39m, in \u001b[36mBaseMixture.score_samples\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    338\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Compute the log-likelihood of each sample.\u001b[39;00m\n\u001b[32m    339\u001b[39m \n\u001b[32m    340\u001b[39m \u001b[33;03mParameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    349\u001b[39m \u001b[33;03m    Log-likelihood of each sample in `X` under the current model.\u001b[39;00m\n\u001b[32m    350\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    351\u001b[39m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m352\u001b[39m X = \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m logsumexp(\u001b[38;5;28mself\u001b[39m._estimate_weighted_log_prob(X), axis=\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cnr/ppcx-app/.venv/lib/python3.12/site-packages/sklearn/utils/validation.py:2975\u001b[39m, in \u001b[36mvalidate_data\u001b[39m\u001b[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[39m\n\u001b[32m   2972\u001b[39m     out = X, y\n\u001b[32m   2974\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params.get(\u001b[33m\"\u001b[39m\u001b[33mensure_2d\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m-> \u001b[39m\u001b[32m2975\u001b[39m     \u001b[43m_check_n_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_estimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2977\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/cnr/ppcx-app/.venv/lib/python3.12/site-packages/sklearn/utils/validation.py:2839\u001b[39m, in \u001b[36m_check_n_features\u001b[39m\u001b[34m(estimator, X, reset)\u001b[39m\n\u001b[32m   2836\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m   2838\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m n_features != estimator.n_features_in_:\n\u001b[32m-> \u001b[39m\u001b[32m2839\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   2840\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mX has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m features, but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2841\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mis expecting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator.n_features_in_\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m features as input.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2842\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: X has 2 features, but GaussianMixture is expecting 4 features as input."
     ]
    }
   ],
   "source": [
    "# display predicted scores by the model as a contour plot\n",
    "x = np.linspace(\n",
    "    np.floor(np.min(features_scaled[:, 0])), np.ceil(np.max(features_scaled[:, 0])), 50\n",
    ")\n",
    "y = np.linspace(\n",
    "    np.floor(np.min(features_scaled[:, 1])), np.ceil(np.max(features_scaled[:, 1])), 50\n",
    ")\n",
    "X, Y = np.meshgrid(x, y)\n",
    "XX = np.array([X.ravel(), Y.ravel()]).T\n",
    "Z = -gmm.score_samples(XX)\n",
    "Z = Z.reshape(X.shape)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "CS = ax.contour(\n",
    "    X,\n",
    "    Y,\n",
    "    Z,\n",
    "    norm=LogNorm(vmin=1.0, vmax=np.ceil(np.max(Z))),\n",
    "    levels=np.logspace(0, 2, 5),\n",
    "    alpha=0.5,\n",
    ")\n",
    "CB = plt.colorbar(CS, shrink=0.8, extend=\"both\")\n",
    "CB.set_label(\"Negative log-likelihood\")\n",
    "ax.scatter(features_scaled[:, 0], features_scaled[:, 1], 0.8)\n",
    "ax.set_xlabel(\"V (scaled)\")\n",
    "ax.set_ylabel(\"angle_rad (scaled)\")\n",
    "ax.set_title(\"Negative log-likelihood predicted by a GMM\")\n",
    "ax.axis(\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9df4fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gmm_log_likelihood_contours(\n",
    "    df, gmm, scaler, variables_names, pair=None, n_grid=80, ax=None, cmap=\"viridis\", alpha=0.5\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot GMM negative log-likelihood contours for any pair of variables in n-dimensional space.\n",
    "    - df: DataFrame with all features\n",
    "    - gmm: fitted GaussianMixture\n",
    "    - scaler: fitted StandardScaler\n",
    "    - variables_names: list of all variable names used for GMM\n",
    "    - pair: tuple/list of two variable names to plot (default: first two)\n",
    "    - n_grid: grid resolution\n",
    "    - ax: matplotlib axis (optional)\n",
    "    - cmap: colormap for contours\n",
    "    - alpha: contour transparency\n",
    "    \"\"\"\n",
    "    import matplotlib.colors as mcolors\n",
    "\n",
    "    if pair is None:\n",
    "        pair = variables_names[:2]\n",
    "    assert len(pair) == 2, \"pair must be a tuple/list of two variable names\"\n",
    "    idx1, idx2 = [variables_names.index(v) for v in pair]\n",
    "    \n",
    "    # Prepare grid in the selected 2D space\n",
    "    data_scaled = scaler.transform(df[variables_names])\n",
    "    xlim = (np.percentile(data_scaled[:, idx1], 1), np.percentile(data_scaled[:, idx1], 99))\n",
    "    ylim = (np.percentile(data_scaled[:, idx2], 1), np.percentile(data_scaled[:, idx2], 99))\n",
    "    x = np.linspace(*xlim, n_grid)\n",
    "    y = np.linspace(*ylim, n_grid)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    grid = np.zeros((n_grid * n_grid, data_scaled.shape[1]))\n",
    "    # Fill grid with mean values for all other variables\n",
    "    grid[:] = np.mean(data_scaled, axis=0)\n",
    "    grid[:, idx1] = X.ravel()\n",
    "    grid[:, idx2] = Y.ravel()\n",
    "    # Compute negative log-likelihood\n",
    "    Z = -gmm.score_samples(grid)\n",
    "    Z = Z.reshape(X.shape)\n",
    "\n",
    "    # Plot\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(7, 6))\n",
    "    else:\n",
    "        fig = ax.figure\n",
    "    levels = np.linspace(np.percentile(Z, 5), np.percentile(Z, 99), 8)\n",
    "    CS = ax.contour(\n",
    "        X,\n",
    "        Y,\n",
    "        Z,\n",
    "        levels=levels,\n",
    "        cmap=cmap,\n",
    "        alpha=alpha,\n",
    "        linewidths=1.5,\n",
    "        norm=mcolors.Normalize(vmin=levels[0], vmax=levels[-1]),\n",
    "    )\n",
    "    CB = fig.colorbar(CS, ax=ax, shrink=0.8, extend=\"both\")\n",
    "    CB.set_label(\"Negative log-likelihood\")\n",
    "    # Scatter the data points\n",
    "    ax.scatter(data_scaled[:, idx1], data_scaled[:, idx2], s=2, c=\"k\", alpha=0.3, label=\"Data\")\n",
    "    ax.set_xlabel(f\"{pair[0]} (scaled)\")\n",
    "    ax.set_ylabel(f\"{pair[1]} (scaled)\")\n",
    "    ax.set_title(f\"GMM Negative log-likelihood: {pair[0]} vs {pair[1]}\")\n",
    "    ax.axis(\"tight\")\n",
    "    ax.legend(loc=\"upper right\", fontsize=9, frameon=True)\n",
    "    ax.grid(True, alpha=0.2)\n",
    "    plt.tight_layout()\n",
    "    return ax\n",
    "\n",
    "# Example usage:\n",
    "# plot_gmm_log_likelihood_contours(df_features, gmm, scaler, variables_names, pair=(\"V\", \"angle_rad\"))\n",
    "# plot_gmm_log_likelihood_contours(df_features, gmm, scaler, variables_names, pair=(\"x\", \"y\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67650847",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ppcx-app (3.12.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
